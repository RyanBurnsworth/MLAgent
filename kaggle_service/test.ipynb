{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d476366",
   "metadata": {},
   "source": [
    "# Dataset Loading and Inspection\n",
    "\n",
    "Dataset Title: <title>\n",
    "Dataset Subtitle: <subtitle>\n",
    "Description: <description>\n",
    "\n",
    "Problem: Build a predictive model to identify the target variable in this tabular dataset. The goal of this notebook is to load the data, inspect its structure, identify the target, and prepare a concise dataset summary for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb151e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f267dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset metadata (fill with actual values from the provided dataset metadata)\n",
    "dataset_metadata = {\n",
    "    'Title': '<title>',\n",
    "    'Subtitle': '<subtitle>',\n",
    "    'Description': '<description>',\n",
    "    'Dataset files': ['<datasets>']\n",
    "}\n",
    "\n",
    "# Normalize to list\n",
    "files = dataset_metadata.get('Dataset files', [])\n",
    "if isinstance(files, str):\n",
    "    files = [files]\n",
    "train_path = None\n",
    "test_path = None\n",
    "if isinstance(files, list) and len(files) > 0:\n",
    "    train_path = files[0]\n",
    "    if len(files) > 1:\n",
    "        test_path = files[1]\n",
    "\n",
    "def load_dataset_from_paths(paths):\n",
    "    if not paths:\n",
    "        raise ValueError('No dataset paths provided.')\n",
    "    last_err = None\n",
    "    for p in paths:\n",
    "        try:\n",
    "            if not isinstance(p, str) or p.strip() == '':\n",
    "                continue\n",
    "            if not os.path.exists(p):\n",
    "                print(f'Warning: path not found: {p}')\n",
    "                continue\n",
    "            ext = os.path.splitext(p)[1].lower()\n",
    "            if ext in ['.csv', '.tsv', '.txt']:\n",
    "                return pd.read_csv(p)\n",
    "            elif ext in ['.xlsx', '.xls']:\n",
    "                return pd.read_excel(p)\n",
    "            elif ext in ['.json']:\n",
    "                return pd.read_json(p)\n",
    "            else:\n",
    "                return pd.read_csv(p)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f'Error reading {p}: {e}')\n",
    "            continue\n",
    "    raise FileNotFoundError('None of the dataset files could be loaded. Last error: ' + str(last_err))\n",
    "\n",
    "# Load datasets\n",
    "df = None\n",
    "test_df = None\n",
    "try:\n",
    "    if train_path:\n",
    "        df = load_dataset_from_paths([train_path])\n",
    "    else:\n",
    "        raise ValueError('No training dataset file specified.')\n",
    "except Exception as e:\n",
    "    print('Failed to load training dataset:', e)\n",
    "\n",
    "if test_path:\n",
    "    try:\n",
    "        test_df = load_dataset_from_paths([test_path])\n",
    "        print('Validation dataset loaded:', test_df.shape)\n",
    "    except Exception as e:\n",
    "        print('Failed to load validation dataset:', e)\n",
    "\n",
    "if df is not None:\n",
    "    print('Training dataset loaded:', df.shape)\n",
    "else:\n",
    "    print('Training dataset not loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print('Training dataset shape:', df.shape)\n",
    "    print('Columns:', df.columns.tolist())\n",
    "    print('Data types:\\n', df.dtypes)\n",
    "    missing_per_column = df.isnull().sum()\n",
    "    print('Missing values per column:\\n', missing_per_column)\n",
    "\n",
    "    # Target detection\n",
    "    target_candidates = ['target','label','class','y']\n",
    "    target_col = None\n",
    "    for c in df.columns:\n",
    "        if str(c).lower() in target_candidates:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        target_col = df.columns[-1]\n",
    "    print('Identified target column:', target_col)\n",
    "\n",
    "    print('First few rows:')\n",
    "    display(df.head())\n",
    "\n",
    "    print('Statistical summary (all columns):')\n",
    "    display(df.describe(include='all'))\n",
    "\n",
    "    # Build dataset summary\n",
    "    import io\n",
    "    buf = io.StringIO()\n",
    "    df.info(buf=buf)\n",
    "    info_str = buf.getvalue()\n",
    "\n",
    "    dataset_summary = {\n",
    "        'shape': df.shape,\n",
    "        'columns': df.shape[1],\n",
    "        'missing_values': bool(df.isnull().any().any()),\n",
    "        'sample_data': df.head().to_dict(orient='records'),\n",
    "        'statistical_summary': df.describe(include='all').to_dict(),\n",
    "        'info': info_str\n",
    "    }\n",
    "else:\n",
    "    dataset_summary = {\n",
    "        'shape': None,\n",
    "        'columns': 0,\n",
    "        'missing_values': False,\n",
    "        'sample_data': [],\n",
    "        'statistical_summary': {},\n",
    "        'info': ''\n",
    "    }\n",
    "\n",
    "print('Dataset summary prepared.')\n",
    "print(dataset_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42040475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_device():\n",
    "    device = 'cpu'\n",
    "    # PyTorch check\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "            return device\n",
    "    except Exception:\n",
    "        pass\n",
    "    # TensorFlow check\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            device = 'cuda'\n",
    "            return device\n",
    "    except Exception:\n",
    "        pass\n",
    "    return device\n",
    "\n",
    "device = detect_device()\n",
    "print('Detected device:', device)\n",
    "\n",
    "# Configure environment to use GPU if available\n",
    "if device == 'cuda':\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Update dataset_summary with device info if available\n",
    "try:\n",
    "    dataset_summary['device'] = device\n",
    "except NameError:\n",
    "    dataset_summary = {'device': device}\n",
    "\n",
    "print('Device configured for future ML tasks:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf542f6",
   "metadata": {},
   "source": [
    "- The dataset has been loaded and examined.\n",
    "- A summary dictionary named dataset_summary has been created containing shape, column count, missing values flag, a sample of the data, a statistical description, and the dataset info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe84128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final dataset summary as JSON for easy use in ML pipelines\n",
    "import json\n",
    "print(json.dumps(dataset_summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
